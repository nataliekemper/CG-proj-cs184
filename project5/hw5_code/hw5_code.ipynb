{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd80772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to install \"gprof2dot\" \n",
    "import io\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.io\n",
    "import sklearn.model_selection\n",
    "import sklearn.tree\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcc065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5  # a small number     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5311c6",
   "metadata": {},
   "source": [
    "# Question 4: Decision Trees for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e82d43",
   "metadata": {},
   "source": [
    "## 4.1: Implement Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a4cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None, m=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "        # adding max number of features m\n",
    "        self.m = m\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        # TODO: implement information gain function\n",
    "        # H(S) - H(after)\n",
    "    \n",
    "        splitl = np.where(X <= thresh)\n",
    "        splitr = np.where(X > thresh)\n",
    "    \n",
    "        # Splitting the labels based on the threshold \n",
    "        sl = y[splitl]\n",
    "        sr = y[splitr]\n",
    "        \n",
    "        HS = DecisionTree.entropy(y)\n",
    "        HSL = DecisionTree.entropy(sl)\n",
    "        HSR = DecisionTree.entropy(sr)\n",
    "        \n",
    "        H_after = ((len(sl) * HSL) + (len(sr) * HSR)) / (len(sl) + len(sr))\n",
    "        \n",
    "        return (HS - H_after)\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        # TODO: implement gini impurity function\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        # TODO: implement entropy function\n",
    "        H = 0\n",
    "        sorted_y, unique_counts = np.unique(y, return_counts = True)\n",
    "        num_classes = len(sorted_y)\n",
    "        for i in unique_counts:\n",
    "            pc = i / num_classes\n",
    "            product = pc * np.log2(pc)\n",
    "            H += product\n",
    "        return H\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0:\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node\n",
    "            # splits.\n",
    "            \n",
    "            # picking m random features out of total d features and selecting only those features for X\n",
    "            data = X\n",
    "            if self.m:\n",
    "                m_feature_ids = np.random.choice(X.shape[1], size = self.m, replace = False)\n",
    "                X = data[:, m_feature_ids]\n",
    "            else:\n",
    "                X = data\n",
    "\n",
    "            thresh = np.array([\n",
    "                np.linspace(np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=10)\n",
    "                for i in range(X.shape[1])\n",
    "            ])\n",
    "            \n",
    "            if self.m:\n",
    "                for i in range(self.m):\n",
    "                    gains.append([self.information_gain(X[:, i], y, t) for t in thresh[i, :]])\n",
    "            else:\n",
    "                for i in range(X.shape[1]):\n",
    "                    gains.append([self.information_gain(X[:, i], y, t) for t in thresh[i, :]])\n",
    "\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            \n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            if self.m:\n",
    "                self.split_idx = m_feature_ids[self.split_idx]\n",
    "            \n",
    "            # splitting on the original data\n",
    "            X0, y0, X1, y1 = self.split(data, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b908d0d",
   "metadata": {},
   "source": [
    "## 4.2 Implement Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d8b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, m_depth = 3, n = 200, features = None):\n",
    "        self.n = n\n",
    "        self.m_depth = m_depth\n",
    "        self.decision_trees = [\n",
    "            DecisionTree(max_depth = m_depth, feature_labels = features) for i in range(self.n)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO: implement function\n",
    "        for n in range(self.n):\n",
    "            row = X.shape[0]\n",
    "            sample = np.random.choice(np.arange(row), size = row, replace = True)\n",
    "            X_train = X[sample]\n",
    "            y_train = y[sample]\n",
    "            self.decision_trees[n].fit(X_train, y_train)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: implement function   \n",
    "        y_hats = []\n",
    "        for i in range(self.n):\n",
    "            pred = self.decision_trees[i].predict(X)\n",
    "            y_hats.append(pred)\n",
    "\n",
    "        return np.round(np.mean(y_hats, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cd1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(BaggedTrees):\n",
    "    def __init__(self, m_depth, features, n=200, m=1): \n",
    "        # TODO: implement function\n",
    "        self.params = params\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTree(max_depth = m_depth, feature_labels = features, m = m) for i in range(self.n)]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f43007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostedRandomForest(RandomForest):\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data\n",
    "        self.a = np.zeros(self.n)  # Weights on decision trees\n",
    "        # TODO: implement function\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: implement function\n",
    "        pass\n",
    "\n",
    "\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == ''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == '-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack([np.array(data, dtype=float), np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        for i in range(data.shape[-1]):\n",
    "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
    "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
    "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
    "\n",
    "    return data, onehot_features\n",
    "\n",
    "\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", sklearn.model_selection.cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [(features[term[0]], term[1]) for term in counter.most_common()]\n",
    "        print(\"First splits\", first_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ee2e6",
   "metadata": {},
   "source": [
    "## 4.3 Describe Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2c823",
   "metadata": {},
   "source": [
    "1.) The process to clean the data consisted of removing rows where labels (whether they survived or not) were completely missing. This ensured that every row would have a corresponding label. In addition, hotcoding allowed categorical variables such as gender or embarked to be mapped to binary vectors of zeros and ones. This helped deal with the catagorical features. In addition, to account for missing data values, we could use the mode of each feature and input that into each missing data value.\n",
    "\n",
    "2.) I used the maximum depth as my stopping criterion. A depth that is too high can result in overfitting, so I had to make sure to use a depth that would not increase the bias too much. \n",
    "\n",
    "3.) I added a m value to my decision tree class to choose m features before splitting between features. Additionally, since my Random Forest inherited my BaggedTrees class, I simply created a list of decision trees (a forest if you will) using m features. For my BaggedTrees class, I sampled with replacement and trained on each sample. I had my forest pick the the optimal sample from my BaggedTrees class. \n",
    "\n",
    "4.) I did not do anything special to optimize speed. \n",
    "\n",
    "5.) No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b993dc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (b): preprocessing the titanic dataset\n",
      "Features: ['pclass', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'male', 'female', 'S', 'C', 'Q']\n",
      "Train/test size: (999, 14) (310, 14)\n",
      "\n",
      "\n",
      "Part 0: constant classifier\n",
      "Accuracy 0.6166166166166166\n",
      "\n",
      "\n",
      "Part (a-b): simplified decision tree\n",
      "Predictions [0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "Part (c): sklearn's decision tree\n",
      "Cross validation [0.795      0.825      0.805      0.755      0.74371859]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = \"titanic\"\n",
    "    params = {\n",
    "        \"max_depth\": 5,\n",
    "        # \"random_state\": 6,\n",
    "        \"min_samples_leaf\": 10,\n",
    "    }\n",
    "    N = 100\n",
    "\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data\n",
    "        path_train = './dataset/titanic/titanic_training.csv'\n",
    "        data = genfromtxt(path_train, delimiter=',', dtype=None, encoding=None)\n",
    "        path_test = './dataset/titanic/titanic_test_data.csv'\n",
    "        test_data = genfromtxt(path_test, delimiter=',', dtype=None, encoding=None)\n",
    "        y = data[1:, -1]  # label = survived\n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "        labeled_idx = np.where(y != '')[0]\n",
    "\n",
    "        y = np.array(y[labeled_idx])\n",
    "        y = y.astype(float).astype(int)\n",
    "\n",
    "\n",
    "        print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "        X, onehot_features = preprocess(data[1:, :-1], onehot_cols=[1, 5, 7, 8])\n",
    "        X = X[labeled_idx, :]\n",
    "        Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "        assert X.shape[1] == Z.shape[1]\n",
    "        features = list(data[0, :-1]) + onehot_features\n",
    "\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "            \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "            \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "            \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = './dataset/spam/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "    print(\"Features:\", features)\n",
    "    print(\"Train/test size:\", X.shape, Z.shape)\n",
    "\n",
    "    print(\"\\n\\nPart 0: constant classifier\")\n",
    "    print(\"Accuracy\", 1 - np.sum(y) / y.size)\n",
    "\n",
    "    # Basic decision tree\n",
    "    print(\"\\n\\nPart (a-b): simplified decision tree\")\n",
    "    dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "    dt.fit(X, y)\n",
    "    print(\"Predictions\", dt.predict(Z)[:100])\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"\\n\\nPart (c): sklearn's decision tree\")\n",
    "    clf = sklearn.tree.DecisionTreeClassifier(random_state=0, **params)\n",
    "    clf.fit(X, y)\n",
    "    evaluate(clf)\n",
    "    out = io.StringIO()\n",
    "\n",
    "    # You may want to install \"gprof2dot\"\n",
    "    sklearn.tree.export_graphviz(\n",
    "        clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "    graph = pydot.graph_from_dot_data(out.getvalue())\n",
    "    pydot.graph_from_dot_data(out.getvalue())[0].write_pdf(\"%s-tree.pdf\" % dataset)\n",
    "\n",
    "    # TODO: implement and evaluate!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79873ab",
   "metadata": {},
   "source": [
    "## 4.4 Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c3cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(array1, array2, num):\n",
    "    merged_list = list(zip(array1, array2))\n",
    "    \n",
    "    shuffled = random.sample(merged_list, len(merged_list))\n",
    "    \n",
    "    unfiltered_training, unfiltered_val = shuffled[num:], shuffled[:num]\n",
    "    \n",
    "    X_training_data, y_training_labels = zip(*unfiltered_training)\n",
    "    X_validation_data, y_validation_labels = zip(*unfiltered_val)\n",
    "    \n",
    "    return np.asarray(X_training_data), np.asarray(y_training_labels), np.asarray(X_validation_data), np.asarray(y_validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf8361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spamData = scipy.io.loadmat(\"dataset/spam/spam_data.mat\")\n",
    "spam_trainingData = spamData['training_data']\n",
    "spam_trainingLabels = spamData['training_labels']\n",
    "spam_test = spamData['test_data']\n",
    "\n",
    "spam_training_data, spam_training_labels, spam_val_data, spam_val_labels = shuffle_data(\n",
    "    spam_trainingData, spam_trainingLabels, int(spam_trainingData.shape[0] * 0.2))\n",
    "\n",
    "feat = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "            \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "            \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "            \"ampersand\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0227eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for SPAM using a Decision Tree: 0.790719696969697.\n",
      "Validation accuracy for SPAM using a Decision Tree: 0.7935606060606061.\n",
      "Training accuracy for SPAM using a Random Forest: 0.7831439393939394.\n",
      "Validation accuracy for SPAM using a Random Forest: 0.7945075757575758.\n"
     ]
    }
   ],
   "source": [
    "decisionTree = DecisionTree(max_depth = 6)\n",
    "randomForest = RandomForest(m_depth = 5, features = feat, n=200, m=4)\n",
    "\n",
    "decisionTree.fit(spam_training_data, spam_training_labels)\n",
    "randomForest.fit(spam_training_data, spam_training_labels)\n",
    "\n",
    "spam_training_pred = decisionTree.predict(spam_training_data)\n",
    "spam_val_pred = decisionTree.predict(spam_val_data)\n",
    "spam_training_pred_forest = randomForest.predict(spam_training_data)\n",
    "spam_val_pred_forest = randomForest.predict(spam_val_data)\n",
    "\n",
    "# Training accuracies \n",
    "spam_training_acc = np.sum(spam_training_pred == spam_training_labels.flatten()) / spam_training_data.shape[0]\n",
    "spam_training_acc_forest = np.sum(spam_training_pred_forest == spam_training_labels.flatten()) / spam_training_data.shape[0]\n",
    "\n",
    "# Validation accuracies \n",
    "spam_val_acc = np.sum(spam_val_pred == spam_val_labels.flatten()) / spam_val_data.shape[0]\n",
    "spam_val_acc_forest = np.sum(spam_val_pred_forest == spam_val_labels.flatten()) / spam_val_data.shape[0]\n",
    "\n",
    "print(f\"Training accuracy for SPAM using a Decision Tree: {spam_training_acc}.\")\n",
    "print(f\"Validation accuracy for SPAM using a Decision Tree: {spam_val_acc}.\")\n",
    "\n",
    "print(f\"Training accuracy for SPAM using a Random Forest: {spam_training_acc_forest}.\")\n",
    "print(f\"Validation accuracy for SPAM using a Random Forest: {spam_val_acc_forest}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "128bd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the titanic data\n",
    "path_train = './dataset/titanic/titanic_training.csv'\n",
    "data = genfromtxt(path_train, delimiter=',', dtype=None, encoding=None)\n",
    "path_test = './dataset/titanic/titanic_test_data.csv'\n",
    "test_data = genfromtxt(path_test, delimiter=',', dtype=None, encoding=None)\n",
    "y = data[1:, -1]  # label = survived\n",
    "class_names = [\"Died\", \"Survived\"]\n",
    "labeled_idx = np.where(y != '')[0]\n",
    "\n",
    "y = np.array(y[labeled_idx])\n",
    "y = y.astype(float).astype(int)\n",
    "\n",
    "X, onehot_features = preprocess(data[1:, :-1], onehot_cols=[1, 5, 7, 8])\n",
    "X = X[labeled_idx, :]\n",
    "Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "assert X.shape[1] == Z.shape[1]\n",
    "features = list(data[0, :-1]) + onehot_features\n",
    "\n",
    "titanicData = X\n",
    "titanicLabels = y\n",
    "titanicFeatures = features\n",
    "titanicTest = Z\n",
    "\n",
    "titanic_training_data, titanic_training_labels, titanic_val_data, titanic_val_labels = shuffle_data(\n",
    "    titanicData, titanicLabels, int(titanicData.shape[0] * 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc36ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for Titanic using a Decision Tree: 0.65625.\n",
      "Validation accuracy for Titanic using a Decision Tree: 0.5778894472361809.\n",
      "Training accuracy for Titanic using a Random Forest: 0.78375.\n",
      "Validation accuracy for Titanic using a Random Forest: 0.7487437185929648.\n"
     ]
    }
   ],
   "source": [
    "decisionTree = DecisionTree(max_depth = 5)\n",
    "randomForest = RandomForest(m_depth = 6, features = titanicFeatures, n=200, m=4)\n",
    "\n",
    "decisionTree.fit(titanic_training_data, titanic_training_labels)\n",
    "randomForest.fit(titanic_training_data, titanic_training_labels)\n",
    "\n",
    "titanic_training_pred = decisionTree.predict(titanic_training_data)\n",
    "titanic_val_pred = decisionTree.predict(titanic_val_data)\n",
    "titanic_training_pred_forest = randomForest.predict(titanic_training_data)\n",
    "titanic_val_pred_forest = randomForest.predict(titanic_val_data)\n",
    "\n",
    "# Training accuracies \n",
    "titanic_training_acc = np.sum(titanic_training_pred == titanic_training_labels.flatten()) / titanic_training_data.shape[0]\n",
    "titanic_training_acc_forest = np.sum(titanic_training_pred_forest == titanic_training_labels.flatten()) / titanic_training_data.shape[0]\n",
    "\n",
    "# Validation accuracies \n",
    "titanic_val_acc = np.sum(titanic_val_pred == titanic_val_labels.flatten()) / titanic_val_data.shape[0]\n",
    "titanic_val_acc_forest = np.sum(titanic_val_pred_forest == titanic_val_labels.flatten()) / titanic_val_data.shape[0]\n",
    "\n",
    "print(f\"Training accuracy for Titanic using a Decision Tree: {titanic_training_acc}.\")\n",
    "print(f\"Validation accuracy for Titanic using a Decision Tree: {titanic_val_acc}.\")\n",
    "\n",
    "print(f\"Training accuracy for Titanic using a Random Forest: {titanic_training_acc_forest}.\")\n",
    "print(f\"Validation accuracy for Titanic using a Random Forest: {titanic_val_acc_forest}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25eaf173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1 # Ensures that the index starts at 1\n",
    "    df.to_csv(\"titanicPrediction2.csv\", index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c2ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_model_forest = DecisionTree(max_depth = 17)\n",
    "spam_model_forest.fit(spamData[\"training_data\"], spamData[\"training_labels\"])\n",
    "spamPredictions = spam_model_forest.predict(spamData[\"test_data\"])\n",
    "results_to_csv(spamPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc6c87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanicModel = RandomForest(m_depth = 6, features = titanicFeatures, n=300, m=4)\n",
    "titanicModel.fit(titanicData, titanicLabels)\n",
    "titanicPredictions = titanicModel.predict(titanicTest)\n",
    "results_to_csv(titanicPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef62d1",
   "metadata": {},
   "source": [
    "Kaggle Username: nadaleek\n",
    "\n",
    "SPAM Score: 0.80394\n",
    "\n",
    "Titanic Score: 0.73548"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66cf62",
   "metadata": {},
   "source": [
    "## 4.5 Writeup the Spam Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c04b3d",
   "metadata": {},
   "source": [
    "drug > 1e-5\n",
    "\n",
    "exclamation > 1e-5\n",
    "\n",
    "semicolon < 1e-5\n",
    "\n",
    "perscription < 1e-5\n",
    "\n",
    "spam > 1e-5\n",
    "\n",
    "money > 1e-5\n",
    "\n",
    "Therefore, the prediction is spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68c96d",
   "metadata": {},
   "source": [
    "business > 1e-5\n",
    "\n",
    "message > 1e-5\n",
    "\n",
    "pain < 1e-5\n",
    "\n",
    "ampersand < 1e-5\n",
    "\n",
    "other < 1e-5\n",
    "\n",
    "Therefore, this prediction is ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd1fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 1 done\n",
      "Depth 2 done\n",
      "Depth 3 done\n",
      "Depth 4 done\n",
      "Depth 5 done\n",
      "Depth 6 done\n",
      "Depth 7 done\n",
      "Depth 8 done\n",
      "Depth 9 done\n",
      "Depth 10 done\n",
      "Depth 11 done\n",
      "Depth 12 done\n",
      "Depth 13 done\n",
      "Depth 14 done\n",
      "Depth 15 done\n",
      "Depth 16 done\n",
      "Depth 17 done\n",
      "Depth 18 done\n",
      "Depth 19 done\n",
      "Depth 20 done\n",
      "Depth 21 done\n",
      "Depth 22 done\n",
      "Depth 23 done\n",
      "Depth 24 done\n",
      "Depth 25 done\n",
      "Depth 26 done\n",
      "Depth 27 done\n",
      "Depth 28 done\n",
      "Depth 29 done\n",
      "Depth 30 done\n",
      "Depth 31 done\n",
      "Depth 32 done\n",
      "Depth 33 done\n",
      "Depth 34 done\n",
      "Depth 35 done\n",
      "Depth 36 done\n",
      "Depth 37 done\n",
      "Depth 38 done\n",
      "Depth 39 done\n",
      "Depth 40 done\n"
     ]
    }
   ],
   "source": [
    "spam_training_data, spam_training_labels, spam_val_data, spam_val_labels = shuffle_data(\n",
    "    spam_trainingData, spam_trainingLabels, int(spam_trainingData.shape[0] * 0.2))\n",
    "\n",
    "depths = np.arange(1, 41)\n",
    "val_acc = []\n",
    "for depth in depths:\n",
    "    decisionTree = DecisionTree(max_depth = depth)\n",
    "    decisionTree.fit(spam_training_data, spam_training_labels)\n",
    "    prediction = decisionTree.predict(spam_val_data)\n",
    "    spam_val_acc = np.sum(prediction == spam_val_labels.flatten()) / spam_val_data.shape[0]\n",
    "    val_acc.append(spam_val_acc)\n",
    "    print(f\"Depth {depth} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2a74135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal depth is 23\n"
     ]
    }
   ],
   "source": [
    "plt.plot(depths, val_acc, label=\"Validation Data\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy vs Depth\")\n",
    "plt.show()\n",
    "\n",
    "max_val = max(val_acc)\n",
    "max_depth = val_acc.index(max_val)\n",
    "print(f\"The optimal depth is {max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006f866",
   "metadata": {},
   "source": [
    "## 4.6 Writeup for the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeaad34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanicModel = DecisionTree(max_depth = 3)\n",
    "mod = titanicModel.fit(titanicData, titanicLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c891b58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This DecisionTree instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m20\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitanicModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtitanicFeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_export.py:180\u001b[0m, in \u001b[0;36mplot_tree\u001b[0;34m(decision_tree, max_depth, feature_names, class_names, label, filled, impurity, node_ids, proportion, rounded, precision, ax, fontsize)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_tree\u001b[39m(\n\u001b[1;32m     79\u001b[0m     decision_tree,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m ):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124;03m\"\"\"Plot a decision tree.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    The sample counts that are shown are weighted with any sample_weights that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     exporter \u001b[38;5;241m=\u001b[39m _MPLTreeExporter(\n\u001b[1;32m    183\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m    184\u001b[0m         feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m         fontsize\u001b[38;5;241m=\u001b[39mfontsize,\n\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exporter\u001b[38;5;241m.\u001b[39mexport(decision_tree, ax\u001b[38;5;241m=\u001b[39max)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1222\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1218\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1219\u001b[0m     ]\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This DecisionTree instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = sklearn.tree.plot_tree(titanicModel, feature_names = titanicFeatures)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b356bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
